\documentclass[a4paper]{article}
\usepackage{Sweave}
\usepackage[margin=2cm]{geometry}
\usepackage[round]{natbib}
\usepackage{url}
\usepackage{hyperref}
\usepackage{listings}

\let\code=\texttt
\newcommand{\acronym}[1]{\textsc{#1}}
\newcommand{\class}[1]{\mbox{\textsf{#1}}}
\newcommand{\pkg}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\fkt}[1]{\code{#1()}}
\newcommand{\todo}[1]{\begin{center}\code{<TODO: #1>}\end{center}}    
\newcommand{\field}[1]{\code{\$#1}} 
 
\sloppy
%% \VignetteIndexEntry{Introduction to the tm.plugin.webmining Package}
\SweaveOpts{prefix.string=webmining} 
\SweaveOpts{include=FALSE}


\begin{document}

<<Init_hidden,echo=FALSE,eval=T, results=hide>>=
library(tm)
library(tm.plugin.webmining)
data(yahoonews)
options(width = 60)
@
 
\title{Short Introduction to \pkg{tm.plugin.webmining}}
\author{Mario Annau\\
		\texttt{mario.annau@gmail.com}}

\maketitle
   
\abstract{
This vignette gives a short introduction to \pkg{tm.plugin.webmining} which
facilitates the retrieval of textual data from the web. The main focus of
\pkg{tm.plugin.webmining} is the retrieval of web content from structured news
feeds in the \proglang{XML} (\proglang{RSS}, \proglang{ATOM}) and
\proglang{JSON} format. Additionally, retrieval and extraction of
\proglang{HTML} documents is implemented. Numerous data sources are currently
supported through public feeds/APIs, including Google-- and Yahoo! News,
Reuters and the New York Times.
}

  
\section{Getting Started}
After package installation we make the functionality of
\pkg{tm.plugin.webmining} available through

<<echo=T, eval=F>>=
library(tm)
library(tm.plugin.webmining)
@

\pkg{tm.plugin.webmining} depends on numerous packages, most
importantly \pkg{tm} by \cite{hornik:Feinerer+Hornik+Meyer:2008} for text
mining capabilities and data structures.
\pkg{RCurl} functions are used for web data retrieval and \pkg{XML} for the 
extraction of \proglang{XML}/\proglang{HTML} based feeds.
As a first experiment, we can retrieve a \class{(Web-)Corpus} using data from
Yahoo! News and the search query \code{"Microsoft"}:

<<echo=T, eval=F>>=
yahoonews <- WebCorpus(YahooNewsSource("Microsoft"))
@ 

Users already familiar with \pkg{tm}
will notice the different function call \fkt{WebCorpus} for corpus construction. Like
\pkg{tm}'s \fkt{Corpus} constructor it takes a \class{(Web-)Source} object as
input and constructs a \class{(Web-)Corpus} object.
A Review of the object's \fkt{class}

<<echo=T, eval=T>>=
class(yahoonews)
@ 

reveals, that \class{WebCorpus} is directly derived from \class{Corpus} and adds
further functionality to it. It can therefore be used like a "normal"
\class{Corpus} using \pkg{tm}'s text mining capabilities.

<<echo=T, eval=T>>=
yahoonews
@ 

Under the hood, a call of \fkt{YahooNewsSource} retrieves a data feed from
Yahoo! News and pre--parses its contents.
Subsequently, \fkt{WebCorpus} extracts (meta--)data from the \class{WebSource}
object and also downloads and extracts the actual main content
of the news item (most commonly an \proglang{HTML}--Webpage).
In effect, it implements a two--step procedure to

\begin{enumerate}
\item Download meta data from the feed (through \class{WebSource})
\item Download and extract main content for the feed item (through
\class{WebCorpus})
\end{enumerate}

These procedures ensure that the resulting \class{WebCorpus} not only includes
a rich set of meta data but also the full main text content for text mining
purposes. An examination of the meta data for the first element in the corpus
is shown below.

<<echo=F, eval=T>>=
# Little hack to restrict output width
meta(yahoonews[[1]], "description") <- 
		paste(substring(meta(yahoonews[[1]], "description"), 1, 70), "...", sep = "")
meta(yahoonews[[1]], "id") <- 
		paste(substring(meta(yahoonews[[1]], "id"), 1, 70), "...", sep = "")
meta(yahoonews[[1]], "origin") <- 
		paste(substring(meta(yahoonews[[1]], "origin"), 1, 70), "...", sep = "")
@
<<echo=T, eval=T>>=
meta(yahoonews[[1]])
@

For a Yahoo! News \class{TextDocument} we get useful meta--data like
\code{DateTimeStamp}, \code{Description}, \code{Heading}, \code{ID} and
\code{Origin}. The main content, as specified in the \code{Origin} of a
\class{TextDocument} can be examined as follows (shortened for output):

<<echo=F, eval=T>>=
# Little hack to restrict output length
content(yahoonews[[1]]) <- 
		paste(substring(yahoonews[[1]], 1, 100), "...", sep = "")
@
<<echo=T, eval=T>>=
yahoonews[[1]]
@

It has been extracted from an unstructured \proglang{HTML} page and freed from
ads and sidebar content by \pkg{boilerpipeR}'s \fkt{DefaultExtractor}. To view the
entire corpus main content also consider \fkt{inspect} (output omitted):

<<echo=T, eval=F>>=
inspect(yahoonews)
@

\section{Implemented Sources}
\begin{table}[t]
  \begin{center}
    \input{tables/sources}
  \end{center}
  \caption{Overview of implemented \class{WebSources} listing the maximum number
  of items per feed, a descriptive URL, if authentification is necessary (x
  for yes) and the feed format.}
  \label{tab:sources}
\end{table}

All currently implemented (web--)sources are listed on Table~\ref{tab:sources}.
The following commands show, how to use the implemented Sources. If available,
the search query/stock ticker \code{Microsoft} has been used. Since Reuters News
only offers a predefined number of channels we selected \code{businessNews}. 

<<echo=T, eval=F>>=
googleblogsearch <- WebCorpus(GoogleBlogSearchSource("Microsoft"))
googlefinance <- WebCorpus(GoogleFinanceSource("NASDAQ:MSFT"))
googlenews <- WebCorpus(GoogleNewsSource("Microsoft"))
nytimes <- WebCorpus(NYTimesSource("Microsoft", appid = nytimes_appid))
reutersnews <- WebCorpus(ReutersNewsSource("businessNews"))
yahoofinance <- WebCorpus(YahooFinanceSource("MSFT"))
yahooinplay <- WebCorpus(YahooInplaySource())
yahoonews <- WebCorpus(YahooNewsSource("Microsoft"))
@

\section{Extending/Updating Corpora}
Most data feeds only contain 20--100 feed items. A text corpus of such a small
size may not be sufficient for text mining purposes. For that reason,
the \fkt{corpus.update} method has been implemented. In a nutshell, it first
downloads a feed's meta data, checks which items are new (as determined by the meta--data
ID field) and finally downloads the main content of new web documents. Since
most time of \class{WebCorpus} construction is spend downloading the main content of
corpus items, this procedures ensures a more efficient and faster
\class{WebCorpus}--update. \\
The Yahoo! News corpus can now simply be updated:

<<echo=T, eval=F>>=
yahoonews <- corpus.update(yahoonews)
@

To continously update a \class{WebCorpus} a scheduled task/cron job could be set
up which runs \fkt{corpus.update} in a script.
\newpage

\section{Conclusion}
This vignette has given a short introduction to \pkg{tm.plugin.webmining}, a
package to retrieve textual data from the web. Although
\pkg{tm.plugin.webmining} has been tested for the retrieval of 10000+ items per
feed it is generally not recommended to start massive feed downloads due to
memory-- and \pkg{RCurl} restrictions. For this purpose, web scraping 
frameworks like Scrapy (\url{scrapy.org}), Heritrix (\url{crawler.archive.org})
or Nutch (\url{nutch.apache.org}) are much better suited.
\\
Keeping these issues in mind, \pkg{tm.plugin.webmining} is well suited for the
retrieval and processing of small to medium sized text corpora. By using the
full meta data and textual contents, quite interesting text mining experiments
can be done using the full capabilities of the \pkg{tm} package. 


\bibliographystyle{plainnat}
\bibliography{references}



\end{document}
